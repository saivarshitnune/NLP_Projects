{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOo9WrSpjLYqNMji8cpODGN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saivarshitnune/NLP_Projects/blob/Varshith/search_question_service.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "from typing import Any\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from api.main.utility.helpers.greetings_detection import detect_greeting\n",
        "from api.main.utility.helpers.keywords_helper import extract_keywords\n",
        "from sentence_transformers import util\n",
        "from api.initializer import logger_instance, model, embeddings_load\n",
        "from api.main.utility.helpers.profanity_filter import profanity_detection\n",
        "from spellchecker import SpellChecker\n",
        "from api.main.utility.helpers.spell_correction import extract_unique_words, extract_dictionary\n",
        "import copy\n",
        "\n",
        "inference_data_timestamp = embeddings_load.current_timestamp\n",
        "df_embeddings = None\n",
        "\n",
        "logger = logger_instance.get_logger(__name__)\n",
        "\n",
        "if embeddings_load.exists:\n",
        "    df_embeddings = copy.deepcopy(embeddings_load.dataframe)\n",
        "    df_list = np.array(df_embeddings[\"embedding\"].to_list())\n",
        "    list_of_all_questions = df_embeddings[\"questions\"].tolist()\n",
        "\n",
        "    words = extract_unique_words(list_of_all_questions)\n",
        "    correction_dict = extract_dictionary()\n",
        "\n",
        "    spell = SpellChecker()\n",
        "    spell.word_frequency.load_words(words)\n",
        "\n",
        "class SearchQuestionService(object):\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    async def find_question(input_question: str) -> Any:\n",
        "        global df_embeddings\n",
        "        global df_list\n",
        "        global inference_data_timestamp\n",
        "        global words\n",
        "        global spell\n",
        "        global correction_dict\n",
        "        global list_of_all_questions\n",
        "\n",
        "        if df_embeddings is not None or embeddings_load.exists:\n",
        "            if inference_data_timestamp != embeddings_load.current_timestamp:\n",
        "                logger.info('Loading Updated Dataframe in Memory')\n",
        "                df_embeddings = copy.deepcopy(embeddings_load.dataframe)\n",
        "                df_list = np.array(df_embeddings['embedding'].to_list())\n",
        "                list_of_all_questions = df_embeddings['questions'].tolist()\n",
        "                words = extract_unique_words(list_of_all_questions)\n",
        "                correction_dict = extract_dictionary()\n",
        "                spell = SpellChecker()\n",
        "                spell.word_frequency.load_words(words)\n",
        "                inference_data_timestamp = embeddings_load.current_timestamp\n",
        "\n",
        "        # Remove punctuation and normalize input_question\n",
        "        input_question = input_question.lower().split()\n",
        "        counter = 0\n",
        "\n",
        "        while counter < len(input_question):\n",
        "            if input_question[counter].endswith('?') and input_question[counter] != '?':\n",
        "                input_question[counter] = input_question[counter][:-1]\n",
        "                input_question.insert(counter + 1, '?')\n",
        "            elif input_question[counter].endswith('.') and input_question[counter] != '.':\n",
        "                input_question[counter] = input_question[counter][:-1]\n",
        "                input_question.insert(counter + 1, '.')\n",
        "            else:\n",
        "                counter += 1\n",
        "        del counter\n",
        "\n",
        "        # Helper function to check if a word exists in the correction dictionary\n",
        "        def check_word(word):\n",
        "            for key, value in correction_dict.items():\n",
        "                if word in value:\n",
        "                    return key\n",
        "            return None\n",
        "\n",
        "        # Correct the input question\n",
        "        corrected_inp = []\n",
        "\n",
        "        for i in input_question:\n",
        "            dict_check = check_word(i)\n",
        "\n",
        "            if i in ['a', 'i']:\n",
        "                corrected_inp.append(i)\n",
        "            elif dict_check is not None:\n",
        "                corrected_inp.append(dict_check)\n",
        "            elif i in words:\n",
        "                corrected_inp.append(i)\n",
        "            elif len(i) == 1:\n",
        "                corrected_inp.append(i)\n",
        "            else:\n",
        "                temp = spell.correction(i)\n",
        "                corrected_inp.append(temp if temp is not None else i)\n",
        "\n",
        "        input_question = ' '.join(str(e) for e in corrected_inp)\n",
        "        del corrected_inp\n",
        "\n",
        "        if input_question == '':\n",
        "            return [], {\"is_profane\": False, \"is_greeting\": False}\n",
        "\n",
        "        question_embedding = model.encode(input_question, convert_to_tensor=True)\n",
        "        cosine_score = util.cos_sim(df_list, question_embedding)\n",
        "        df_embeddings['score'] = [round(float(score), 2) for score in cosine_score]\n",
        "\n",
        "        result = df_embeddings[df_embeddings['score'] >= 0.70].reset_index(drop=True)\n",
        "        if len(result) <= 6:\n",
        "            result = df_embeddings[df_embeddings['score'] >= 0.65].reset_index(drop=True)\n",
        "        if len(result) <= 6:\n",
        "            result = df_embeddings[df_embeddings['score'] >= 0.50].reset_index(drop=True)\n",
        "\n",
        "        def stem_keywords(tokens):\n",
        "            stemmer = PorterStemmer()\n",
        "            return [stemmer.stem(y) for y in tokens]\n",
        "\n",
        "        question_keywords_new = await extract_keywords(input_question, method='Rake')\n",
        "        lemmatize_question = await extract_keywords(question_keywords_new, method='Rake_ques')\n",
        "        question_keywords = stem_keywords(lemmatize_question)\n",
        "\n",
        "        is_profane = await profanity_detection(input_question)\n",
        "        is_greeting = await detect_greeting(input_question)\n",
        "\n",
        "        def lemmatize_text(tokens):\n",
        "            x = ast.literal_eval(str(tokens))\n",
        "            lemmatizer = WordNetLemmatizer()\n",
        "            return [lemmatizer.lemmatize(y) for y in x]\n",
        "\n",
        "        def find_keywords(question_keys):\n",
        "            return list(question_keys.intersection(set(question_keywords)))\n",
        "\n",
        "        if len(result) == 0:\n",
        "            return [], {\"is_profane\": is_profane, \"is_greeting\": is_greeting}\n",
        "\n",
        "        result['lematized_keys'] = result[\"keywords_rake\"].apply(lemmatize_text)\n",
        "        result['stemmed_keys'] = result['lematized_keys'].apply(stem_keywords)\n",
        "        result[\"similar_keywords\"] = result['stemmed_keys'].apply(find_keywords)\n",
        "        result[\"keywords_count\"] = result[\"similar_keywords\"].apply(len)\n",
        "        result.sort_values(by=[\"score\", \"keywords_count\"], ascending=[False, False], inplace=True)\n",
        "\n",
        "        result = result.loc[result.groupby(\"mid\")[\"score\"].idxmax()].reset_index(drop=True)\n",
        "        max_count = result[\"keywords_count\"].max()\n",
        "        result = result.loc[result['keywords_count'] >= (max_count - 2)]\n",
        "        result[\"relative_score\"] = result['score'] / result[\"score\"].max()\n",
        "\n",
        "        match = []\n",
        "\n",
        "        for _, row in result.iterrows():\n",
        "            match.append({\n",
        "                \"mid\": row[\"mid\"],\n",
        "                \"score\": row[\"score\"],\n",
        "                \"relative_score\": round(row[\"relative_score\"], 3),\n",
        "                \"keywords\": row[\"similar_keywords\"],\n",
        "                \"keywords_count\": row[\"keywords_count\"],\n",
        "                \"question_digest\": row[\"master_question_digest\"],\n",
        "                \"master_question\": row[\"master_question\"],\n",
        "                \"uuids\": row[\"master_question_uuids\"]\n",
        "            })\n",
        "\n",
        "        if match:\n",
        "            return match, {\"is_profane\": is_profane, \"is_greeting\": False}\n",
        "        else:\n",
        "            return [], {\"is_profane\": is_profane, \"is_greeting\": is_greeting}\n"
      ],
      "metadata": {
        "id": "9YT5odeL0FUq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}